{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96937c2a-67c2-4e17-849a-cab1ee9856b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "train_data = pd.read_csv('./train.csv')\n",
    "test_data = pd.read_csv('./test_x.csv')\n",
    "\n",
    "train_data = train_data.drop(train_data[train_data.familysize > 20].index).reset_index(drop=True)\n",
    "flip_cols = [\"QaA\", \"QdA\", \"QgA\", \"QiA\", \"QnA\", \"QeA\", \"QfA\", \"QkA\", \"QqA\", \"QrA\"]\n",
    "for col in flip_cols:\n",
    "    train_data[col] = 6 - train_data[col]\n",
    "    test_data[col] = 6 - test_data[col]\n",
    "\n",
    "answers = [f'Q{c}A' for c in 'abcdefghijklmnopqrst']\n",
    "times = [f'Q{c}E' for c in 'abcdefghijklmnopqrst']\n",
    "train_data['mach_score'] = train_data[answers].mean(axis=1)\n",
    "test_data['mach_score'] = test_data[answers].mean(axis=1)\n",
    "train_data['total_time'] = np.log1p(train_data[times].sum(axis=1))\n",
    "test_data['total_time'] = np.log1p(test_data[times].sum(axis=1))\n",
    "\n",
    "drop_list = times + ['index', 'hand']\n",
    "train_y = (2 - train_data['voted']).to_numpy().astype(np.float32)\n",
    "train_x_raw = train_data.drop(drop_list + ['voted'], axis=1)\n",
    "test_x_raw = test_data.drop(drop_list, axis=1)\n",
    "\n",
    "cat_cols = ['education', 'engnat', 'married', 'urban', 'age_group', 'gender', 'race', 'religion']\n",
    "num_cols = [c for c in train_x_raw.columns if c not in cat_cols]\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), num_cols),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols)\n",
    "])\n",
    "\n",
    "train_x_scaled = preprocessor.fit_transform(train_x_raw).astype(np.float32)\n",
    "test_x_scaled = preprocessor.transform(test_x_raw).astype(np.float32)\n",
    "\n",
    "train_x_t = torch.tensor(train_x_scaled).to(DEVICE)\n",
    "train_y_t = torch.tensor(train_y).to(DEVICE)\n",
    "test_x_t = torch.tensor(test_x_scaled).to(DEVICE)\n",
    "\n",
    "class SwapNoise(nn.Module):\n",
    "    def __init__(self, prob=0.15):\n",
    "        super().__init__()\n",
    "        self.prob = prob\n",
    "    def forward(self, x):\n",
    "        if not self.training: return x\n",
    "        mask = torch.rand(x.shape, device=x.device) < self.prob\n",
    "        shuffled = x[torch.randperm(x.shape[0], device=x.device)]\n",
    "        x_noised = x.clone()\n",
    "        x_noised[mask] = shuffled[mask]\n",
    "        return x_noised\n",
    "\n",
    "class DAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=1024):\n",
    "        super().__init__()\n",
    "        self.noise = SwapNoise(prob=0.15)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim), nn.BatchNorm1d(hidden_dim), nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.BatchNorm1d(hidden_dim), nn.SiLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.BatchNorm1d(hidden_dim), nn.SiLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(self.noise(x))\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded, encoded\n",
    "\n",
    "class DAE_Classifier(nn.Module):\n",
    "    def __init__(self, dae_encoder, input_dim, dae_dim=1024):\n",
    "        super().__init__()\n",
    "        self.encoder = dae_encoder\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False \n",
    "        combined_dim = input_dim + dae_dim\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 512), nn.BatchNorm1d(512), nn.SiLU(), nn.Dropout(0.4),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.SiLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            encoded = self.encoder(x)\n",
    "        combined = torch.cat([x, encoded], dim=1)\n",
    "        return self.net(combined).squeeze()\n",
    "\n",
    "\n",
    "dae_model = DAE(train_x_t.shape[1]).to(DEVICE)\n",
    "dae_opt = optim.AdamW(dae_model.parameters(), lr=1e-3)\n",
    "dae_crit = nn.MSELoss()\n",
    "\n",
    "dae_loader = DataLoader(TensorDataset(train_x_t), batch_size=512, shuffle=True)\n",
    "for _ in tqdm(range(50)):\n",
    "    dae_model.train()\n",
    "    for (batch_x,) in dae_loader:\n",
    "        dae_opt.zero_grad()\n",
    "        decoded, _ = dae_model(batch_x)\n",
    "        loss = dae_crit(decoded, batch_x)\n",
    "        loss.backward(); dae_opt.step()\n",
    "\n",
    "skf = StratifiedKFold(n_splits=7, shuffle=True, random_state=42)\n",
    "dae_preds = np.zeros((len(test_x_t), 1), dtype=np.float32)\n",
    "\n",
    "for fold, (t_idx, v_idx) in enumerate(skf.split(train_x_scaled, train_y)):\n",
    "    train_loader = DataLoader(TensorDataset(train_x_t[t_idx], train_y_t[t_idx]), batch_size=512, shuffle=True)\n",
    "    valid_loader = DataLoader(TensorDataset(train_x_t[v_idx], train_y_t[v_idx]), batch_size=512)\n",
    "    \n",
    "    model = DAE_Classifier(copy.deepcopy(dae_model.encoder), train_x_t.shape[1]).to(DEVICE)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.05)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1.1], device=DEVICE))\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    fold_pred = np.zeros((len(test_x_t), 1))\n",
    "    \n",
    "    pbar = tqdm(range(60), desc=f'Fold {fold+1}')\n",
    "    for epoch in pbar:\n",
    "        model.train()\n",
    "        for xx, yy in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(xx), yy)\n",
    "            loss.backward(); optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = sum(criterion(model(xx), yy).item() * len(yy) for xx, yy in valid_loader) / len(v_idx)\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                fold_pred = torch.sigmoid(model(test_x_t)).cpu().numpy().reshape(-1, 1)\n",
    "        pbar.set_postfix({'val_loss': f'{best_loss:.4f}'})\n",
    "    \n",
    "    dae_preds += fold_pred / 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029c733b-6757-4f7e-8d1b-25be542e712d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.read_csv(\"./sample_submission.csv\")\n",
    "sub_df.iloc[:, 1:] = 1.0 - dae_preds\n",
    "sub_df.to_csv(\"Model6.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
